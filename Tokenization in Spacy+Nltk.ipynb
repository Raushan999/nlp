{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a621a52-9f6b-42f7-8eb1-8e08bee0e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef43cdd-37dc-4c10-ad22-16dfa08f1814",
   "metadata": {},
   "source": [
    "Spacy is object oriented:\n",
    "\n",
    "Because we created an object and that object (doc) has the property of spacy (doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3707caff-6563-4cb7-a8a6-4100ab893615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hi, My name is rkp. I'm from gaya, bihar"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hi, My name is rkp. I'm from gaya, bihar\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9171ea8c-17f4-445f-ba15-7bf2ee461e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, My name is rkp.\n",
      "I'm from gaya, bihar\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf00d21-cf67-4fbb-98e7-e83c3345fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      ",\n",
      "My\n",
      "name\n",
      "is\n",
      "rkp\n",
      ".\n",
      "I\n",
      "'m\n",
      "from\n",
      "gaya\n",
      ",\n",
      "bihar\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # individual words\n",
    "    for word in sent:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999cda5-b60a-4747-af89-924740aa5428",
   "metadata": {},
   "source": [
    "### nltk\n",
    "#### it is a string processing libraries\n",
    "\n",
    "Does not create object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17ce573d-8eb3-4480-b59a-7cb32718f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cbd1173-c6a7-44f4-bbf3-11334425516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa49670a-6cd6-4db7-8426-b3f99afd68b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'sdfsdf', 'sdkfoe']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"s sdfsdf sdkfoe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9a5b38c-4618-40fb-bc12-88ef2187e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a840f-1259-4891-a6f6-92b79dce6a66",
   "metadata": {},
   "source": [
    "# TOKENIZATION IN `SPACY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d833af4d-ab4f-4229-a68d-fae35df63fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e5aa1a4-19bd-427e-bae7-9bef2bd50a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en') # 'en' is language model\n",
    "doc = nlp(\"I have five rupees:- 8 $90.24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dd8c09e6-7375-47a4-99c6-e2d699ee5b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "five\n",
      "rupees:-\n",
      "8\n",
      "$\n",
      "90.24\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c31009c-6bf6-4e47-a8b7-78590e40d596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[-2].is_currency, doc[-1].is_digit, doc[-1].like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77bcd1f5-f7d7-41bd-b53f-fe4dd6bb4956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('students.txt') as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79a26352-2cff-4123-b2ee-846dfa5f3653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n==================================================\\n\\nName\\tbirth day   \\temail\\n-----\\t------------\\t------\\nVirat   5 June, 1882    virat@kohli.com\\nMaria\\t12 April, 2001  maria@sharapova.com\\nSerena  24 June, 1998   serena@williams.com \\nJoe      1 May, 1997    joe@root.com\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joining the above text as a single text with space as delimiter\n",
    "text = ''.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c28558e6-35e5-4e9b-b16f-728775da88b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[virat@kohli.com, maria@sharapova.com, serena@williams.com, joe@root.com]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the emails from this text:\n",
    "doc = nlp(text)\n",
    "emails = [token for token in doc if token.like_email is True]\n",
    "# for token in doc:\n",
    "#     if token.like_email == True:\n",
    "#         emails.append(token)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e54e8ca4-c0fa-4cab-898d-1184516ee515",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDr. Strange loves pav bhaji of mumbai. Hulk loves chat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sent)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:923\u001b[0m, in \u001b[0;36msents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "50ea7067-0e30-429a-9dd3-890e6fa855d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1d89d062c50>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we'dont have the the sentencizer added to the pipeline.\n",
    "# i.e., the nlp object don't know how to split para into sentences:\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8e63c94f-0851-4a34-9992-cfa8e856a274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Mr. Mrs. Hi I'm here.\n",
      "Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chat\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Mr. Mrs. Hi I'm here. Strange loves pav bhaji of mumbai. Hulk loves chat\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62a6ab-2869-408f-8a97-12fe3209d0c4",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7bc97-4fad-4c7e-ae55-87ac998b1f9d",
   "metadata": {},
   "source": [
    "#### 1. grab all urls from this paragraph using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "96a9b3f0-aa7e-45f9-8baa-8af1cd112967",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eb88efa9-490f-4185-9862-a3ae16081fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[http://www.data.gov/,\n",
       " http://www.science,\n",
       " http://data.gov.uk/.,\n",
       " http://www3.norc.org/gss+website/,\n",
       " http://www.europeansocialsurvey.org/.]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)  # creating object of nlp \n",
    "urls = [token for token in doc if token.like_url is True]\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6e130645-630e-4b32-9d5a-4f560d64f92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff14ae-1d01-4818-8ee9-1cd50435279e",
   "metadata": {},
   "source": [
    "### (2) Extract all money transaction from below sentence along with currency. Output should be,\n",
    "\n",
    "two $\n",
    "\n",
    "500 €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f6a5ad76-44b7-4985-a7d2-620063e20fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1fb3b6e7-a106-4dab-99a0-777fa005b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(transactions)\n",
    "val = [token for token in doc2 if token.like_num is True]\n",
    "sign = [ token for token in doc2 if token.is_currency is True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8907e7-7826-488a-ad3e-4a34d6d7aae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cae0b0-74e4-4948-b548-b886789ce19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
